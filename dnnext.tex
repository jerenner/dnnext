\documentclass[a4paper,11pt]{article}
\pdfoutput=1

\usepackage{jheppub} 
\usepackage[T1]{fontenc} % if needed
\usepackage{amsmath,amssymb}
%\usepackage[numbers,sort]{natbib}
%\hypersetup{colorlinks, citecolor=blue, linkcolor=blue, filecolor=blue, urlcolor=red}

\title{Background rejection in NEXT using deep neural networks}

\author{The NEXT Collaboration\newline}
\author[a,1]{J. Renner,\note{Corresponding author.}}
\author[b]{A. Farbin,}
\author[a]{J. M. Benlloch-Rodr\'{i}guez,}
\author[a]{P. Ferrario,}
\author[a]{J. Mu\~{n}oz Vidal,}
\author[a,2]{J. J. G\'omez-Cadenas,\note{Spokesperson.}}
\author[a]{V. \'{A}lvarez,}
\author[h]{C. D. R. Azevedo,}
\author[c]{F. I. G. Borges,}
\author[a]{S. C\'{a}rcel,}
\author[a]{J. V. Carri\'{o}n,}
\author[d]{S. Cebri\'{a}n,}
\author[a]{A. Cervera,}
\author[c]{C. A. N. Conde,}
\author[a]{J. D\'{i}az,}
\author[p]{M. Diesburg,}
\author[f]{R. Esteve,}
\author[c]{L. M. P. Fernandes,}
\author[h]{A. L. Ferreira,}
\author[c]{E. D. C. Freitas,}
%\author[e]{V. M. Gehman,}
\author[e]{A. Goldschmidt,}
\author[q]{D. Gonz\'{a}lez-D\'{i}az,}
\author[i]{R. M. Guti\'{e}rrez,}
\author[j]{J. Hauptman,}
\author[c]{C. A. O. Henriques,}
\author[k]{J. A. Hernando Morata,}
\author[f]{V. Herrero,}
\author[b]{B. Jones,}
\author[l]{L. Labarga,}
\author[a]{A. Laing,}
\author[p]{P. Lebrun,}
\author[a]{I. Liubarsky,}
\author[a]{N. L\'{o}pez-March,}
\author[a]{D. Lorca,}
\author[i]{M. Losada,}
%\author[f]{A. Mar\'{i},}
\author[r]{J. Mart\'{i}n-Albo,}
\author[k]{G. Mart\'{i}nez-Lema,}
\author[a]{A. Mart\'{i}nez,}
%\author[e]{T. Miller,}
\author[r]{J. Mart\'{i}n-Albo,}
\author[k]{G. Mart\'{i}nez-Lema,}
\author[a]{A. Mart\'{i}nez,}
\author[a]{F. Monrabal,}
%\author[a]{M. Monserrate,}
\author[c]{C. M. B. Monteiro,}
\author[f]{F. J. Mora,}
\author[h]{L. M. Moutinho,}
\author[a]{M. Nebot-Guinot,}
\author[a]{P. Novella,}
\author[b]{D. Nygren,}
\author[p]{A. Para,}
%\author[m]{J. L. P\'{e}rez-Aparicio,}
%\author[l]{J. P\'{e}rez,}
\author[a]{M. Querol,}
\author[n]{L. Ripoll,}
\author[a]{J. Rodr\'{i}guez,}
\author[c]{F. P. Santos,}
\author[c]{J. M. F. dos Santos,}
\author[a]{L. Serra,}
\author[e]{D. Shuman,}
\author[a]{A. Sim\'{o}n,}
\author[o]{C. Sofka,}
\author[a]{M. Sorel,}
%\author[o]{T. Stiegler,}
\author[f]{J. F. Toledo,}
\author[n]{J. Torrent,}
\author[g]{Z. Tsamalaidze,}
\author[h]{J. F. C. A. Veloso,}
%\author[d]{J. A. Villar,}
\author[o]{R. Webb,}
\author[o,3]{J. White,\note{Deceased.}}
\author[o]{R. Webb,}
\author[a]{N. Yahlali,}
\author[i]{H. Yepes-Ram\'{i}rez}

\affiliation[a]{Instituto de F\'{i}sica Corpuscular (IFIC), CSIC \& Universitat de Val\`{e}ncia,\\ 
	Calle Catedr\'{a}tico Jos\'{e} Beltr\'{a}n, 2, Paterna, Valencia, 46980 Spain}
\affiliation[b]{University of Texas at Arlington\\
	701 S. Nedderman Drive, Arlington, Texas, 76019 USA}
\affiliation[c]{Departamento de Fisica, Universidade de Coimbra, \\
	Rua Larga, Coimbra, 3004-516 Portugal}
\affiliation[d]{Lab. de F\'{i}sica Nuclear y Astropart\'{i}culas, Universidad de Zaragoza,\\
	Calle Pedro Cerbuna, 12, Zaragoza, 50009 Spain}
\affiliation[e]{Lawrence Berkeley National Laboratory (LBNL),\\
	1 Cyclotron Road, Berkeley, California, 94720 USA}
\affiliation[f]{Instituto de Instrumentaci\'{o}n para Imagen Molecular (I3M), Universitat Polit\`{e}cnica de Val\`{e}ncia,\\
	Camino de Vera, s/n, Edificio 8B, Valencia, 46022 Spain}
\affiliation[g]{Joint Institute for Nuclear Research (JINR),\\
	Joliot-Curie, 6, Dubna, 141980 Russia}
\affiliation[h]{Institute of Nanostructures, Nanomodelling and Nanofabrication (i3N), Universidade de Aveiro,\\
	Campus de Santiago, Aveiro, 3810-193 Portugal}
\affiliation[i]{Centro de Investigaciones, Universidad Antonio Nari\~{n}o,\\
	Carretera 3 este No. 47A-15, Bogot\'{a}, Colombia}
\affiliation[j]{Department of Physics and Astronomy, Iowa State University,\\
	12 Physics Hall, Ames, Iowa, 50011-3160 U.S.A.}
\affiliation[k]{Instituto Gallego de F\'{i}sica de Altas Energ\'{i}as (IGFAE), Univ. de Santiago de Compostela,\\
	Campus sur, R\'{u}a Xos\'{e} Mar\'{i}a Su\'{a}rez N\'{u}\~{n}ez, s/n, Santiago de Compostela, 15782 Spain}
\affiliation[l]{Departamento de F\'{i}sica Te\'{o}rica, Universidad Aut\'{o}noma de Madrid,\\
	Ciudad Universitaria de Cantoblanco, Madrid, 28049 Spain}
\affiliation[m]{Dpto. de Mec\'{a}nica de Medios Continuos y Teor\'{i}a de Estructuras, Univ. Polit\`{e}cnica de Val\`{e}ncia,\\
	Camino de Vera, s/n, Valencia, 46071 Spain}
\affiliation[n]{Escola Polit\`{e}cnica Superior, Universitat de Girona,\\
	Av. Montilivi, s/n, Girona, 17071 Spain}
\affiliation[o]{Department of Physics and Astronomy, Texas A\&M University,\\
	College Station, Texas, 77843-4242 U.S.A.}
\affiliation[p]{Fermi National Accelerator Laboratory,\\
	Batavia, Illinois, 60510 U.S.A.}
\affiliation[q]{CERN, European Organization for Nuclear Research,\\
	Geneva, 1211 Switzerland}
\affiliation[r]{Department of Physics, University of Oxford,\\
	Parks Road, Oxford, OX1 3PU United Kingdom}

% e-mail addresses: one for each author, in the same order as the authors
\emailAdd{jrenner@ific.uv.es}


\bibliographystyle{plain}
\input{commands}

\abstract{We investigate the potential of using deep learning techniques to reject background 
events in searches for neutrinoless double-beta decay with high pressure xenon time projection 
chambers capable of detailed track reconstruction.  The differences in the topological signatures of background and signal events can be learned by deep neural networks via training over many thousands of events.  These networks can then be used to classify
	further events as signal or background, providing an additional background rejection factor at an acceptable loss of
	efficiency.  This method is found to perform substantially better than previous methods developed based on the use of the
	same topological signatures and has the potential for further improvement.}

\keywords{Neutrinoless double beta decay; deep neural networks; TPC; high-pressure xenon chambers;  Xenon; NEXT-100 experiment}



\begin{document} 
\maketitle
\flushbottom

\section{Introduction}\label{sec:intro}
\noindent Neutrinoless double-beta decay (\bbonu) is a rare nuclear process in which the nuclear charge changes by two units, and the full energy of the decay ($Q_{\beta\beta}$) is 
emitted in the form of two electrons (or positrons).  It is essentially two simultaneous $\beta^{+}$ or $\beta^{-}$ decays in which no neutrinos are emitted.  While two-neutrino double-beta 
decay ($2\nu\beta\beta$), two simultaneous beta decays in which the two neutrinos are present, is a process predicted by the standard model that has been observed in several isotopes, 
\bbonu\,has never been observed, even after over 75 years of experimental effort.  If \bbonu\,does exist, this implies that the neutrino is its own anti-particle, that is, a
a Majorana particle \cite{Schechter_1982}, amongst other important physical implications (see for example \cite{GomezCadenas:2013ue, Cadenas_2012, Avignone_2008}).

The detection of \bbonu\,decay continues to present unique experimental challenges.  For a given isotope, the lifetime of \bbonu\,decay depends on a nuclear matrix element, 
which can be calculated to some uncertainty, and the square of the effective neutrino mass $|m_{\beta\beta}|^2 = |\sum_{i=e,\nu,\tau}U_{ei}^2m_{i}|^2$ which is a combination of the neutrino
masses $m_{i}$ and neutrino mixing matrix elements $U_{ei}$.  The lifetime is relatively shorter for a degenerate neutrino mass hierarchy ($m_1 \sim m_2 \sim m_3$), longer for an inverted 
neutrino mass hierarchy ($m_1 \ll m_2 < m_3$), and longest for a normal mass hierarchy ($m_1 < m_2 \ll m_3$).  Experiments of the current generation contain approximately
100 kg of the candidate isotope and are subject to several tens of counts per year of background events in their region of interest of energy selection near $Q_{\beta\beta}$\cite{GomezCadenas:2013ue}.  These experiments
will be capable of probing only the parameter space corresponding to the degenerate mass hierarchy, perhaps pushing into the inverted hierarchy.  In order to completely cover the parameter 
space of the inverted mass hierarchy, experiments employing candidate isotope masses at the ton-scale with background rates of (at most) few counts per ton-year will be required.  

NEXT (Neutrino Experiment with a Xenon TPC) \cite{Gomez-Cadenas:2014dxa} will search for neutrinoless double-beta decay using a Time Projection Chamber (TPC) filled, in its first stage, with 100 kg of 
high-pressure xenon gas (HPXe), enriched to 90\% in the $^{136}$Xe, and operating at a pressure between 10 and 20 bar.  The detector uses electroluminescence to amplify the signal and optical sensors (photomultipliers and the so called silicon photomultipliers or SiPMs) to readout both the primary scintillation light (S1), defining the start-of-the-interaction and the secondary scintillation (S2) produced by the amplification, in the electroluminescence region of the drift electrons (see figure \ref{fig.SS}). S2, in turn, is recorded by a plane of photomultiplier tubes (PMTs) located behind a transparent cathode (the so-called energy plane), and by a plane containing about 8,000 SiPMs, forming a matrix of pixels spaced at 1 cm pitch (the tracking plane). 
% in which 
%ionization electrons are drifted in a region of relatively low electric field to a narrow region of relatively high electric field, in which they are then accelerated to energies high enough to excite but not 
%ionize the atoms of the xenon gas.  These excitations give rise to the emission of electroluminescent light which is detected by an energy plane of 60 photomulitplier tubes (PMTs) and a tracking 
%plane of about 8000 silicon photomultipliers (SiPMs).  The detected electroluminescent light signal is called (S2), and can be compared in time with the light detected (S1) due to the excitations 
%produced during the initial creation of the ionization track to determine the z-coordinate of its production.  
The pattern of light observed on the tracking plane, located just behind the narrow 
electroluminescent region, can be used to obtain the x-y location of the ionization electrons producing the corresponding S2.  The result, combined with the time at which light is recorded, is a 3D image of the ionization track produced within the 
detector.  The amount of S2 light detected by the energy plane gives an accurate measurement of the energy of the event.  Together this information can be used to identify potential 
\bbonu\,events from the various energy depositions produced in the active volume.

\begin{figure}[!htb]
	\centering
	\includegraphics[width= 0.95\textwidth]{fig/SoftAsymmetric_bound.pdf}
	\caption{Production and detection of S1 and S2 in an asymmetric HPXe TPC (figure from \cite{MartinAlbo_thesis}).} \label{fig.SS}
\end{figure}

In addition to performing a competitive search for \bbonu, NEXT will demonstrate potential 
techniques for operation and background rejection at the ton-scale.  The NEXT collaboration has already built and tested several kg-scale prototypes, NEXT-DBDM \cite{Alvarez:2012kua} and
NEXT-DEMO \cite{Alvarez:2012xda,Alvarez:2012kua,Alvarez:2013gxa,Lorca:2014sra}, which have both demonstrated the excellent energy resolution (extrapolated to 0.5-0.7\% FWHM at
$Q_{\beta\beta}$) obtainable in high pressure xenon gas.  NEXT-DEMO has demonstrated the feasibility of signal/background discrimination based on the topology of reconstructed tracks,
an essential component to identifying \bbonu\,events and rejecting background events (see section \ref{sec:topology}).  The collaboration is currently constructing the first phase of the experiment NEXT-NEW, a 10 kg prototype 
containing about 20\% of the sensors of the 100 kg detector (NEXT-100).  By confirming the ability to obtain good energy resolution and reconstructed tracks at several different scales, NEXT 
intends to confirm the suitability of high pressure xenon gas for operation at the ton-scale.

\section{The NEXT Topological Signature}\label{sec:topology}
One of the most important aspects of NEXT is its ability to image the ionization tracks produced by energetic electrons in its active volume.  Double-beta decay events have a distinct
topological signature, e.g, the presence on the event of two electrons emitted from a common vertex. In a HPXe chamber, the vertex cannot be identified, nor the individual electrons separated, due to the confusing effect of multiple scattering. However, the signature of those ``double-electron'' (or 2e) tracks is substantially different from that of a single electron (1e) produced, for example, by photoelectric or Compton 
conversion of a high-energy gamma ray.  Figure \ref{fig.ETRK2} compares the two types of
tracks. A single electron (figure  \ref{fig.ETRK2}--right) appears in the chamber as a long and relatively smooth track near its beginning and becomes increasingly contorted, due to increasing multiple scattering (as it loses energy ionizing the gas) until the electron comes to a stop, depositing
its final several hundred keV of energy in a relatively smaller volume, which we describe as a ``blob.'' 
%is due to several factors.  First, the ionization density ($dE/dx$) increases rapidly with decreasing energy as the energy of the electron drops below about 1 MeV.  Furthermore, electron multiple scattering, the repeated scattering of the electron throughout the creation of its ionization path, also becomes more intense at lower energies.  Together
%these effects lead to a single-electron track that is long and relatively smoother near its beginning and becomes increasingly contorted until the electron comes to a stop, depositing
%its final several hundred keV of energy in a relatively smaller volume, which we describe as a ``blob.''  
A double-beta event (figure  \ref{fig.ETRK2}--left)  looks like two such tracks beginning at a single vertex.  
%Figure \ref{fig.ETRK2} compares the two types of
%tracks.  

In NEXT is possible to separate signal from backgrounds using this topological signature. The conventional algorithm developed by the collaboration is based in reconstructing an ordered
track, locating the endpoints, and determining whether the energy in those endpoints (blobs) is sufficiently large. Alternatively it is possible to use deep learning techniques, as discussed in this paper.   

\begin{figure}[!htb]
	\centering
	\includegraphics[width= 0.95\textwidth]{fig/TrackSignature.pdf}
	\caption{Monte Carlo simulated \bbonu\,(left) and background (right) events.  The simulation was done in xenon gas at a pressure of 15~bar. Signal events consist of two electrons emanating
		from a common vertex, producing two initially smooth tracks ending in two dense energy depositions (or ``blobs'').  Background events consist of one long track terminating in one
		single ``blob'' (figure from \cite{MartinAlbo_thesis}).} \label{fig.ETRK2}
\end{figure}

\subsection{Monte Carlo Simulation}\label{ssec:NEXT100MC}
This study uses our GEANT4-based \cite{GEANT4} fast Monte Carlo. The simulation models the full geometry of the NEXT-100 detector.  Bot signal and background events were generated. Signal events (\bbonu) were
simulated randomly throughout the active region of the detector, while the leading background events---gamma rays of energies 2.447 MeV and 2.614 MeV corresponding to gammas emitted by daughters of $^{214}$Bi and $^{208}$Tl,
respectively---were shotfrom several different regions (e.g, energy and tracking plane, detector vessel, etc.) within the detector geometry. % confirm these energies with Javi 
The resulting locations and magnitudes of the energy depositions in the active volume were recorded by GEANT4 as hits with a 1 mm minimum step size, and each event was then
analyzed according to the following procedure:

\begin{enumerate}
	\item[1.] A fiducial cut was applied to the true Monte Carlo hits, ensuring that no more than 10 keV of energy was deposited within 2 cm of the edges of the active region.
	%\item[2.] An energy cut was applied to the true Monte Carlo energy, ensuring that all events contained a total deposited energy between 2.3 and 3.0 MeV.
	\item[2.] The total energy of each event was smeared according to a (conservative) detector resolution of 0.7\% FWHM.
	\item[3.] An energy cut was applied ensuring a energy of between 2.4 and 2.5 MeV. %????
	\item[4.] The set consisting of true Monte Carlo hits was converted into one or more subsets of connected voxels.  A voxel was defined to be a small cubical volume of a chosen 
	length, width, and height.  3D space was divided into voxels of the specified size, and the (smeared) energy of each hit was placed in the voxel in which it was located according to its
	true position.  A connected set of voxels, one in which each voxel neighbored at least one other voxel in the set, was called a track, where ``neighboring'' voxels shared at least one face, edge,
	or corner. This ``voxelization of the track'' is a rather good representation of the outcome of the full reconstruction applied to NEXT events (for a discussion see \cite{NEXT_topology}). 
	\item[5.] Only events with exactly one single track were accepted.  This cut effectively suppressed background events accompanied by the emission of x-rays associated to the de-excitation of the xenon atom (e.g, after a photoelectric interaction). 
	\end{enumerate}

%%%%
Table XX shows how the set of cuts described above reduces the signal and the leading backgrounds.  Notice that, in order to characterize the rejection power of the topological signature, a relatively large energy window of 100 keV around \Qbb\ is used. The total rejection power of NEXT will be the combination of the rejection power achieved by the pre-selection (cuts 1-5 above), the topological signature discussed in this paper and a final, stricter energy cut that takes event in a relatively narrow region of interest ROI around \Qbb. See \cite{Martin-Albo:2015rhw} for a detailed discussion.

\subsection{The standard NEXT topological analysis}\label{ssec:TopologicalAnalysis}
After being subject to the cuts described in the previous section, the events were classified as signal or background based on the presence of one or two ``blobs'' of energy in the
reconstructed track.  The procedure followed was similar to the one described in \cite{NEXT_topology, Martin-Albo:2015rhw}.  The track was reconstructed by finding the shortest path between each two 
pairs of connected voxels (using the Breadth First Search (BFS) algorithm), and choosing the longest of these paths to be the ordered path.  Note that such a path may not have included, 
in fact most likely did not include, all voxels in the event.  The first and last voxels in the path
were considered to be the beginning and end of the track.  The energy in all voxels that were located within a given radius $r_b$ of the beginning of the track was summed to give
an energy $E_{b,1}$, and similarly at the end of the track to give an energy $E_{b,2}$.  The histogram of $E_{b,1}$ vs. $E_{b,2}$ is shown in figure \ref{fig.blobcuts} for both signal and
background events.  In this case, the distance between a voxel and the extremes of the track was calculated as 
``along-the-track distance,'' in which the distance between any two voxels was integrated along the reconstructed track (as opposed to computing the Euclidean distance). 
% (How are voxels
%not in the main track treated?)

\begin{figure}[!htb]
	\centering
	\includegraphics[scale = 0.45]{fig/blobcuts_bb0nu_2x2x2_E1vsE2.png}
	\includegraphics[scale = 0.45]{fig/blobcuts_Tl208_2x2x2_E1vsE2.png}
	\caption{Computed blob energies $E_{b,1}$ vs. $E_{b,2}$ for $r_b = 15$ mm for signal (left) and background ($^{208}$Tl right) events.} \label{fig.blobcuts}
\end{figure}

Now by applying a cut selecting signal events with two blobs, mandating that $E_{b,1} > 0.3$ MeV and $E_{b,2} > 0.3$ MeV, we eliminate all but 8.79\% of background ($^{208}$Tl) events 
and keep 84.15\% of signal events.  The same procedure rejected all but 7.80\% of background events due to $^{214}$Bi.


%Results depend of the size of the voxel, which in turn is chosen in the fast simulation to reflect the expected performance of the detector under specific operating conditions. Standard operation with pure Xenon, is characterized by large transverse diffusion, of the order of $\sim 1 cm/\sqrt{m}$~ and also large longitudinal diffusion, of the order of  $\sim 5 mm/\sqrt{m}$. To reflect this fact, the voxels corresponding to the ``standard NEXT configuration'' are chosen to have a size of 10 x 10 x 5 mm3 (see cite--Justo-thesis for discussion). On the other hand, recent work within the NEXT collaboration suggests that diffussion can be substantially reduced by adding small mixtures of quencher gases (cite paper-magnetic-field). We choose a second voxelization, of 2 x 2 x2 mm3 to reflect a "Optimized" case of low diffusion and study how this affects the performance of the topological signature".  PLOTS!

%RESULTS with large and small voxels and CLASSICAL standard cuts. 

\section{Deep Learning}
The use of artificial neural networks to solve problems has been explored since the 1940s.  In recent years, with the dramatic increase in available computing power, the use of computationally
intense neural networks with many inner layers has become feasible.  These neural nets that are many layers deep, called deep neural networks (DNNs), are capable of learning large
amounts of data exhitibing a vast array of features.  This idea of ``deep learning'' has been applied to yield outstanding performance in solving difficult problems such as image and 
speech recognition.

% Details of GoogLeNet; what is a CNN
Neural networks consist of layers of neurons which compute an output value based on one or several input values.  The output $z$ is a function of the weighted sum of the inputs $x_{i}$ plus a bias variable $b$, i.e. $z = f(\sum_{i}w_{i}x_{i} + b)$, where $f$ is called the activation function and $w_{i}$ are the weights of the neuron, one for each input.  The idea is that
with several layers of many neurons connected together, the values of the final (``output'') layer of neurons will correspond to the solution of some problem given the values input to the
initial layer (called the ``input'' layer).  The weights and biases of all neurons in the network together determine the final output value, and so the network must be trained (the weights and
biases must be adjusted) so that the network solves the correct problem.  This is done by using a training dataset, and for each training event, presenting input data to the network, 
examining its resulting output, and adjusting the weights and biases of the network in a manner that minimizes the discrepancy between the output of the final layer $a$ and the expected 
output $y$.  This adjustment procedure is done by computing a cost function which depends on the actual and expected outputs and quantifies the discrepancy between them, computing 
the gradients of the cost function with respect to the weights and biases in all neurons, and changing the weights and biases in a manner that minimizes the cost function.  After many training
iterations, the weights and biases in the network will ideally have converged to values that not only yield the expected output when the network is presented with an event from the training
dataset, but also yields the expected output when presented with similar events not used in training.  The technical
details behind implementing such a scheme mathematically will not be given here but are discussed at length in \cite{Nielsen_2016}.

Recently, multi-layer convolutional neural networks (CNNs) have been identified as a powerful technique for image recognition problems.  These neural networks consist of $m\times n$
convolutional layers - layers of neurons that share a common set of $m\times n$ weights and a bias.  The set of 
weights $+$ biases is called a filter or kernel, and this filter is combined in a multiplicative sum (a convolution) with an $m\times n$ subset of input neurons to give an output value.  The filter
is moved along the image, each time covering a different $m\times n$ subset of input neurons, and the set of output values corresponding to a single filter is called a feature map.  With this
strategy, the net can be trained to identify specific features of an image, and further convolutional layers can be used to analyze the higher-level features encoded in the feature maps output
from previous layers.  Often to reduce the amount of computation and neurons present in deeper layers, max-pooling operations are performed, in which the neuron with maximum output
value in an $m\times n$ window (or ``pool'') is selected, and all others in the pool are discarded.  Such an operation performed on a layer of neurons leads to a new layer of reduced size.
A deep CNN may be constructed from a series of several such convolutional operations and max-pooling operations, along with more conventional fully-connected layers, in which all neurons 
output from the previous layer are connected to the input of each neuron in the fully-connected layer, and other operations not discussed here (see figure \ref{fig.generalnn} for a general 
schematic).

\begin{figure}[!htb]
	\centering
	\includegraphics[scale = 0.6]{fig/general_nn.pdf}
	\caption{Schematic of a deep convolutional neural network for 2-category classification.  The input layer consists of the pixel intensities of an image, possibly in multiple color channels.  The 
		hidden layers consist of several different operations performed on the input neurons - this example shows a 3x3 convolution followed by a 3x3 max-pooling operation, with the resulting 
		neurons input to a fully-connected layer which feeds the two neurons in the output layer.  Rather than reading the values of the activation functions of
		the two neurons in the output layer, the values are converted to two values in a normalized exponential distribution.  These values can then be interpreted as probabilities of classification as
		signal or background.} \label{fig.generalnn}
\end{figure}

In this initial study, we make use of the GoogLeNet \cite{Googlenet}, which is a sophisticated 22-layers-deep convolutional network designed for image recognition.  As GoogLeNet was
designed to classify and identify a wide range of features in a full-color images, a more suitable network is likely to exist for our specific problem of classifying particle tracks based on
topology.  While further exploration of DNN architecture is essential to understanding the problem fully, our main goal in this study will be to show that DNNs can ``learn'' to classify 
NEXT events as signal or background better than previously developed conventional analysis methods.

\section{Event classification with a DNN}
Here we investigate the performance of a DNN in classifying events into two categories, ``signal'' and ``background,'' and compare the results to the conventional analysis described in
section \ref{ssec:TopologicalAnalysis}.  We chose to use the GoogLeNet DNN for this initial study, as its implementation was readily available in the Caffe \cite{jia2014caffe}
deep learning framework along with an interface, DIGITS \cite{DIGITS}, which allows for fast creation of image datasets and facilitates their input to several DNN models.  For each simulated
dataset, GoogLeNet was trained on several tens of thousands of input events on a single GPU (either a GeForce GTX TITAN or GeForce GTX TITAN X, depending on the dataset), and several
thousand additional events as validation.  Each event was input to the net as a .png image consisting of three
color (RGB) channels, one for each of three projections of the 3D voxelized track, (R, G, B) $\rightarrow$ (xy, yz, xz).  This information for a signal event and a background event is
shown in figure \ref{fig.exampleProjs}.

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.36]{fig/plt_h2D_vox_dnn3d_NEXT100_Paolina222_v2x2x2_r200x200x200_0_bg.pdf}
	\includegraphics[scale=0.36]{fig/plt_h2D_vox_dnn3d_NEXT100_Paolina222_v2x2x2_r200x200x200_2_si.pdf}
	\caption{\label{fig.exampleProjs}Projections in xy, yz, and xz for an example background (above) and signal (below) event.}
\end{figure}

\subsection{Analysis of NEXT-100 Monte Carlo}\label{ssec:NEXTMCanalysis}
To compare the ability of the DNN to classify events directly with the performance of the topological analysis of section \ref{ssec:TopologicalAnalysis}, we consider NEXT-100 Monte Carlo 
events that have passed all cuts (1-6) described in section \ref{ssec:NEXT100MC}, with chosen voxel sizes of both 2x2x2 cubic mm and 10x10x5 cubic mm.  For each chosen voxel size, the 
GoogLeNet DNN was trained on 30000 such events (15000 signal and 15000 background) over 30 epochs, 
and an independent set of events was used as a test dataset (6000 in the 2x2x2 voxel case and 4000 in the 10x10x5 case, split evenly between signal and background).  Because these were 
the exact same events as those used in the previous
topological analysis, the classification into signal and background from both analyses can be compared directly.  The results are shown in table \ref{tbl.DNNcomparison}.  The DNN
analysis performs better than the conventional analysis, but there is still potential room for improvement.  

\begin{table}[!htb]
	\begin{center}
		\caption[DNN analysis summary]{\label{tbl.DNNcomparison}Comparison of conventional and DNN-based analyses.  The comparison shows, for a given percentage of signal events
			correctly classified, the number of background events accepted (mistakenly classified as signal).}
		\begin{tabular}{rcc}
			\\
			\textbf{Analysis} & \textbf{Signal eff. (\%)} & \textbf{B.G. accepted (\%)}\\
			\hline
			DNN analysis (2x2x2 voxels) & 84.16 & 5.00\\
			Conventional analysis (2x2x2 voxels) & 84.15 & 8.79\\
			\hline
			DNN analysis (10x10x5 voxels) & 74.23 & 8.30\\
			Conventional analysis (10x10x5 voxels) & 74.22 & 11.37\\
		\end{tabular}
	\end{center}
\end{table}

Because the output layer of the DNN gives a probability that a given event is signal and a probability that it is background, and these probabilities add to 1, a threshold may be 
chosen for determining whether an event is classified as signal or background.  It can be simply chosen as 50\%, meaning the category with greatest probability is the classification of the
event, or it can be varied to reject further background at the expense of signal efficiency.  Figure \ref{fig_svsb} shows the corresponding pairs of signal efficiency and background 
rejection produced by variation of this threshold, while for the values reported in table \ref{tbl.DNNcomparison} the threshold was chosen such that the signal efficiency matched that reported in 
the conventional analysis.

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.6]{fig/sigvsbg_DNN.pdf}
	\caption{\label{fig_svsb}Signal efficiency vs. background rejection for DNN analysis of voxelized (2x2x2 and 10x10x5 cubic mm), single-track NEXT-100 Monte Carlo events.}
\end{figure}

\subsection{Evaluating the DNN analysis}\label{ssec:DNNeval}
We now ask what is causing some significant fraction of the events to be misclassified in the analysis described in section \ref{ssec:NEXTMCanalysis}.  A similar analysis was run on 
several different Monte Carlo datasets generated with differing physics effects with the hopes of better understanding where potential improvements could be made.

A simple Monte Carlo, which we call the ``toy Monte Carlo'' or ``toy MC,'' was designed to produce ionization tracks of single-electron and two-electron events with a fixed energy
considering minimal physical effects.  Discrete energy depositions were produced with a step size less than 1 mm according to the average stopping power $dE/dx$ as tabulated by
NIST \cite{NIST_mac} for xenon at 15 atm.  Electron multiple scattering was modeled by casting random gaussian numbers to determine the angles $\theta_{x}$ and $\theta_{y}$ of deflection, 
assuming the particle's direction of travel is $\hat{\mathbf{z}}$, and the angles $\theta_{x}$ and $\theta_{y}$ are the angles between the scattered direction and $\hat{\mathbf{z}}$ projected on 
the x-z and y-z planes respectively, as

\begin{equation}\label{eqn_mscat}
\sigma^{2}(\theta_{x,y}) = \frac{13.6\,\,\mathrm{MeV}}{\beta p}\sqrt{dz/L_{0}}\bigl[1 + 0.038\ln(dz/L_{0})\bigr].
\end{equation}

\noindent where $dz$ is the thickness of xenon travelled in this step, $L_{0}$ is the radiation length in xenon, $p$ is the electron momentum in MeV/c, and $\beta = v/c$, assuming $c = 1$.

Such tracks were generated and voxelized similar to the procedure described in point 5 of section \ref{ssec:NEXT100MC}.  Note that no ``single-track'' cut was necessary because no
physics generating a secondary track was implemented.  Also no energy smearing was performed.  For background events, the track generation began with a single electron emitted in a
random direction with energy 2.4578 MeV, while for signal events, this energy was shared equally between two electrons emitted in random directions from a single initial vertex.  The DNN
classified the resulting events with nearly 100\% accuracy.  Several modifications were then made to attempt to gain insight into the physics causing the lower classification observed in the
more detailed Monte Carlo tracks.  First, a realistic distribution of energies of the two electrons in signal events \cite{Ponkratenko_2000} was used, and later the magnitude of the multiple scattering was doubled (the prefactor 13.6 in equation \ref{eqn_mscat} was increased to 27.2).  The electron energy distribution caused a loss of about 1\% in average accuracy, and the
increased multiple scattering an additional 1\%.  However, even the two effects together were not enough to account for the inaccuracy of about 7\% observed in the events produced by the
full Monte Carlo.

Therefore an alternate GEANT-based Monte Carlo was run in which the detector geometry was not present, and background events (single electrons) and signal events (two electrons emitted
from a common vertex with a realistic \bbonu\,energy distribution) were generated in a large box of pure xenon gas at 15 bar.  An insignificant amount of energy smearing was applied to these
events, and they were then subject to the same voxelization procedure and single-track cut as described in section \ref{ssec:NEXTMCanalysis} point 5.  Under these controlled conditions, many 
events could be generated with different aspects of the physics switched on/off.  It was confirmed that with the same physics as that used in the NEXT-100 Monte Carlo, the DNN classified events 
with similar accuracy as before.  Disabling bremsstrahlung seemed to have little effect on the accuracy.  Disabling energy fluctuations in GEANT had some small effect (approx. 1\% increase in
accuracy), and disabling the production of secondaries (disallowing the production of secondaries with a range of less than 20 cm) had a more sigificant effect (approx. 2.5\% increase in accuracy),
though still did not yield accuracy similar to the toy MC datasets.  It was found that disabling both energy fluctuations and the production of secondaries gave accuracies similar to the toy MC
(about 98\%), and therefore we can conclude these are the physical effects that are often deceiving the DNN.

A summary of the Monte Carlo simulations run and the classification accuracies obtained is given in table \ref{tbl.DNNsummary}.  

\begin{table}[!htb]
	\begin{center}
		\caption[DNN analysis summary]{\label{tbl.DNNsummary}Summary of DNN analysis for different Monte Carlo datasets.  The accuracy was computed assuming that the classification
			of the DNN corresponded to the category (signal or background) with the higher ($> 50$\%) probability.  In each case, approximately 15000 signal and 15000 background events were
			used in the training procedure, and between 2000-3000 signal and 2000-3000 background events independent of the training set were used to determine the accuracy.}
		\begin{tabular}{rc}
			\\
			\textbf{Run description} & \textbf{Avg. accuracy} (\%)\\
			\hline
			toy MC, ideal & 99.8\\
			toy MC, realistic \bbonu\,distribution & 98.9\\
			Xe box GEANT4, no secondaries, no E-fluctuations & 98.3\\
			Xe box GEANT4, no secondaries, no E-fluctuations, no brem. & 98.3\\
			toy MC, realistic \bbonu\,distribution, double multiple scattering & 97.8\\
			Xe box GEANT4, no secondaries & 94.6\\
			NEXT-100 GEANT4 & 93.1\\
			Xe box GEANT4, no E-fluctuations & 93.0\\
			Xe box, no brem. & 92.4\\
			Xe box, all physics & 92.1
		\end{tabular}
	\end{center}
\end{table}

(Do we want to discuss further the ``zoology'' of events misclassified by the DNN?  The production of a large secondary (``delta'') electron near the beginning of the track in a single-electron event, for example, could produce an event that looks almost indistinguishable from a double-beta event in topology.)

\section{Conclusions}
A DNN-based analysis using GoogLeNet with just three projections seems to be capable of outperforming, in signal/background separation, a conventional analysis based on locating energy ``blobs'' at the ends of the tracks produced by energetic electrons.  The production of secondaries coupled with energy fluctuations in energy deposition seems to be the principle cause of accuracy loss in the DNN analysis, at least in the case of high-resolution ($\sim$ 2 mm) reconstruction.  Future studies geared toward developing a DNN targeted on the problem at hand, and attempting to extract information on what characteristics of the tracks it is ``learning,'' would lead to a more complete understanding of the possibilities and limitations of a DNN analysis.

\acknowledgments

This work was supported by the European Research Council under the Advanced Grant 339787-NEXT and the Ministerio de Econom\'{i}a y Competitividad of Spain under Grants CONSOLIDER-Ingenio 2010 CSD2008-0037 (CUP), FPA2009-13697-C04-04, FPA2009-13697-C04-01, FIS2012-37947-C04-01, FIS2012-37947-C04-02, FIS2012-37947-C04-03, and FIS2012-37947-C04-04.  JR acknowledges support from a Fulbright Junior Research Award.

\bibliography{dnnext}

\end{document}
